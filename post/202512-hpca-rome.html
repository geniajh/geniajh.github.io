<!DOCTYPE html>
<html lang='ko'>
  <head>
    <title>Jung Ho Ahn - Post 2025/12/02</title>
    <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
      new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
        j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-NPDWL48G');
    </script>
    <!-- End Google Tag Manager -->
    <!-- Metadata goes here -->
    <meta charset='UTF-8'/>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel='stylesheet' href='../styles.css'/>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inconsolata:wght@200..900&family=Noto+Sans+KR:wght@100..900&family=Noto+Serif+KR:wght@200..900&family=Source+Sans+3:ital,wght@0,200..900;1,200..900&display=swap" rel="stylesheet">
    <link rel="stylesheet" media='(prefers-color-scheme: dark)' href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.css"/>
    <link rel="stylesheet" media='(prefers-color-scheme: light)' href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.css"/>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>

<!-- and it's easy to individually load additional languages -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/sql.min.js"></script>

<script>hljs.highlightAll();</script>
  </head>
  <body>
    <!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-NPDWL48G"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->
    <!-- Content goes here -->
    <div class='header-container'>
      <header>
        <h1><a href='../'>Jung Ho Ahn</a></h1>
        <p><time datetime='2025-12-02'>Dec 2nd, 2025</p>
      </header>
    </div>

    <nav>
      <ul>
        <li>2025/12/02</li>
        <li><a href='202511-hpca-ive.html'>2025/11/30</a></li>
        <li><a href='202511-snp-rowhammer.html'>2025/11/14</a></li>
        <li><a href='202509-prospective-students.html'>2025/09/16</a></li>
        <li><a href='202509-ai-class.html'>2025/09/01</a></li>
        <li><a href='202508-asplos-cheddar.html'>2025/08/20</a></li>
        <li><a href='202508-google-scholar.html'>2025/08/17</a></li>
        <li><a href='202508-cal-ssd.html'>2025/08/11</a></li>
        <li><a href='202507-cal-prac.html'>2025/07/10</a></li>
        <li><a href='202505-cal-cosmos.html'>2025/05/23</a></li>
        <li>
          <a href='https://dblp.org/pid/a/JungHoAhn.html' target='_blank'><img src='https://dblp.org/img/dblp.icon.192x192.png' class='dblp'/></a>
          <a href='https://scholar.google.co.kr/citations?user=hjIKJrwAAAAJ&hl=en' target='_blank'><img src='https://upload.wikimedia.org/wikipedia/commons/c/c7/Google_Scholar_logo.svg' class='googlescholar'/></a>
          <a href='https://orcid.org/0000-0003-1733-1394' target='_blank'><img src='https://upload.wikimedia.org/wikipedia/commons/e/e6/ORCID_iD_32x32.svg' class='orcid'/></a>
          <a href='https://kr.linkedin.com/in/jung-ho-ahn-9876748' target='_blank'><img src='https://upload.wikimedia.org/wikipedia/commons/e/e8/Linkedin-logo-blue-In-square-40px.png' class='linkedin'/></a>
        </li>
      </ul>
    </nav>

    <div class='blog-container'>
      <section>
        <h1>
          A <span class='rh'>RoMe</span> paper accepted at <span class='hpca'>HPCA</span> 2026 (along with three other papers)
        </h1>
      </section>

      <section>
        <div class='section-main'>
          <p>Delighted to announce that we have a second paper accepted at <a href='https://conf.researchr.org/home/hpca-2026' target='_blank'><span class='hpca'>HPCA</span> 2026</a>: <span class='rh'>RoMe</span>.</p>

          <dl>
            <dt>The Problem</dt>
            <dd>There is a fundamental granularity mismatch in modern AI hardware.
              While HBM is the de facto standard for AI accelerators, its 32-byte access granularity has remained unchanged for over a decade.
              In contrast, Large Language Models (LLMs) operate on hidden states and weight matrices that require streaming contiguous blocks ranging from kilobytes to megabytes.</dd>

            <dt>The Solution</dt>
            <dd>We propose <span class='rh'>RoMe</span> (<span class='rh'>Ro</span>w-granularity-access <span class='rh'>Me</span>mory system).
              <span class='rh'>RoMe</span> shifts DRAM access to row granularity, effectively eliminating the complex column and bank group structures required for fine-grained access.</dd>
          </dl>

          <p>By simplifying the interface, we can free up command/address pins and repurpose them to create additional memory channels.
          The result is a 12.5% increase in bandwidth with minimal hardware overhead. Think of it as a &ldquo;huge-page only&rdquo; system for the terabyte-scale memory era.</p>

          <p>Full credit goes to my students: Hwayong Nam, Seungmin Baek, Jumin Kim, and <a href='../index.html#mjkim'>Michael Jaemin Kim</a>.</p>

          <p>See you all in Sydney!</p>
        </div>
      </section>

       <section>
        <h2><a class='paper' href='https://arxiv.org/abs/2512.01541'><span class='rh'>RoMe</span></a></h2>
        <div class='section-main'>
          <h3>Title</h3>
          <p><span class='rh'>RoMe</span>: Row Granularity Access Memory System for Large Language Models</p>

          <h3>Authors</h3>
          <p>Hwayong Nam, Seungmin Baek, Jumin Kim, <a href='../index.html#mjkim'>Michael Jaemin Kim</a>, and Jung Ho Ahn</p>
     
          <h3>Abstract</h3>
          <p>Modern HBM-based memory systems have evolved over generations while retaining cache line granularity accesses. Preserving this fine granularity necessitated the introduction of bank groups and pseudo channels. These structures expand timing parameters and control overhead, significantly increasing memory controller scheduling complexity. Large language models (LLMs) now dominate deep learning workloads, streaming contiguous data blocks ranging from several kilobytes to megabytes per operation. In a conventional HBM-based memory system, these transfers are fragmented into hundreds of 32B cache line transactions. This forces the memory controller to employ unnecessarily intricate scheduling, leading to growing inefficiency.</p>
          <p>To address this problem, we propose <span class='rh'>RoMe</span>. <span class='rh'>RoMe</span> accesses DRAM at row granularity and removes columns, bank groups, and pseudo channels from the memory interface. This design simplifies memory scheduling, thereby requiring fewer pins per channel. The freed pins are aggregated to form additional channels, increasing overall bandwidth by 12.5% with minimal extra pins. <span class='rh'>RoMe</span> demonstrates how memory scheduling logic can be significantly simplified for representative LLM workloads, and presents an alternative approach for next-generation HBM-based memory systems achieving increased bandwidth with minimal hardware overhead.</p>
        </div>
      </section>


      <footer>
        <div class='index'><a href='202511-hpca-ive.html'>&lt; prev</a> | next &gt;</div> 
        <a href='https://dblp.org/pid/a/JungHoAhn.html' target='_blank'><img src='https://dblp.org/img/dblp.icon.192x192.png' class='dblp'/></a>
        <a href='https://scholar.google.co.kr/citations?user=hjIKJrwAAAAJ&hl=en' target='_blank'><img src='https://upload.wikimedia.org/wikipedia/commons/c/c7/Google_Scholar_logo.svg' class='googlescholar'/></a>
        <a href='https://orcid.org/0000-0003-1733-1394' target='_blank'><img src='https://upload.wikimedia.org/wikipedia/commons/e/e6/ORCID_iD_32x32.svg' class='orcid'/></a>
        <a href='https://kr.linkedin.com/in/jung-ho-ahn-9876748' target='_blank'><img src='https://upload.wikimedia.org/wikipedia/commons/e/e8/Linkedin-logo-blue-In-square-40px.png' class='linkedin'/></a>
        <div class='copyright'>&copy; Jung Ho Ahn</div>
      </footer>

    </div>
  </body>
</html>


