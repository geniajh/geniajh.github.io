<!DOCTYPE html>
<html lang='en'>
  <head>
    <title>Jung Ho Ahn - Post 2025/08/11</title>
    <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
      new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
        j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-NPDWL48G');
    </script>
    <!-- End Google Tag Manager -->
    <!-- Metadata goes here -->
    <meta charset='UTF-8'/>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel='stylesheet' href='../styles.css'/>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inconsolata:wght@200..900&family=Noto+Sans+KR:wght@100..900&family=Noto+Serif+KR:wght@200..900&family=Source+Sans+3:ital,wght@0,200..900;1,200..900&display=swap" rel="stylesheet">
    <link rel="stylesheet" media='(prefers-color-scheme: dark)' href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.css"/>
    <link rel="stylesheet" media='(prefers-color-scheme: light)' href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.css"/>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>

<!-- and it's easy to individually load additional languages -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/sql.min.js"></script>

<script>hljs.highlightAll();</script>
  </head>
  <body>
    <!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-NPDWL48G"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->
    <!-- Content goes here -->
    <div class='header-container'>
      <header>
        <h1><a href='../'>Jung Ho Ahn</a></h1>
        <p><time datetime='2025-08-11'>August 11th, 2025</p>
      </header>
    </div>

    <nav>
      <ul>
        <li><a href='202508-google-scholar.html'>2025/08/17</a></li>
        <li>2025/08/11</li>
        <li><a href='202507-cal-prac.html'>2025/07/10</a></li>
        <li><a href='202505-cal-cosmos.html'>2025/05/23</a></li>
        <li><a href='202504-fantastic-four.html'>2025/04/05</a></li>
        <li><a href='202503-asplos-marionette.html'>2025/03/30</a></li>
        <li><a href='202502-eurosys-pet.html'>2025/02/01</a></li>
        <li><a href='202412-hall-of-fame.html'>2024/12/14</a></li>
        <li><a href='202411-hpca-anaheim.html'>2024/11/04</a></li>
        <li><a href='202411-micro-hof.html'>2024/11/01</a></li>
        <li>
          <a href='https://dblp.org/pid/a/JungHoAhn.html' target='_blank'><img src='https://dblp.org/img/dblp.icon.192x192.png' class='dblp'/></a>
          <a href='https://scholar.google.co.kr/citations?user=hjIKJrwAAAAJ&hl=en' target='_blank'><img src='https://upload.wikimedia.org/wikipedia/commons/c/c7/Google_Scholar_logo.svg' class='googlescholar'/></a>
          <a href='https://orcid.org/0000-0003-1733-1394' target='_blank'><img src='https://upload.wikimedia.org/wikipedia/commons/e/e6/ORCID_iD_32x32.svg' class='orcid'/></a>
          <a href='https://kr.linkedin.com/in/jung-ho-ahn-9876748' target='_blank'><img src='https://upload.wikimedia.org/wikipedia/commons/e/e8/Linkedin-logo-blue-In-square-40px.png' class='linkedin'/></a>
        </li>
      </ul>
    </nav>

    <div class='blog-container'>
      <section>
        <h1>
          Another paper accepted at <a href='https://www.computer.org/csdl/journal/ca' target='_blank'>IEEE Computer Architecture Letters</a>
        </h1>
      </section>

      <section>
        <div class='section-main'>
          <p>Offloading less-used experts from <a href='202408-micro-duplex.html'>Mixture-of-Experts (MoE) LLMs</a> to SSDs is an attractive strategy for running large models on memory-constrained devices. However, our research highlights a critical consideration: <em>energy efficiency</em>.</p>

          <p>We found that the energy required to fetch expert weights from an SSD can become a significant bottleneck, potentially dominating the total power consumption during inference.
          <a class='paper' href='https://arxiv.org/abs/2508.06978'>Our paper</a> quantifies this energy trade-off and concludes that SSD offloading is most beneficial under specific scenarios:</p>

          <ul>
            <li>Systems with low batch sizes, such as those found on mobile or edge devices.</li>
            <li>If future SSDs achieve a >10&times; improvement in energy efficiency (reaching &sim;10 pJ/bit).</li>
          </ul>

          <p>Furthermore, emerging techniques like <a href='https://research.google/blog/looking-back-at-speculative-decoding/'>speculative decoding</a> can increase the &lsquo;effective&rsquo; batch size even on edge devices, further challenging the viability of this offloading approach. Our work emphasizes that the role of storage hardware in MoE systems must be evaluated prudently.</p>
       </div>
      </section>

      <section>
        <h2>Implications of SSD offloading for LLM MoE on energy efficiency</h2>
        <div class='section-main'>
          <h3>Title</h3>
          <p><a class='paper' href='https://ieeexplore.ieee.org/abstract/document/11095626'>SSD Offloading for LLM Mixture-of-Experts Weights Considered Harmful in Energy Efficiency</a></p>

          <h3>Authors</h3>
          <p>Kwanhee Kyung, Sungmin Yun, Jung Ho Ahn</p>

          <h3>Abstract</h3>
          <p>Large Language Models (LLMs) applying Mixture-of-Experts (MoE) scale to trillions of parameters but require vast memory, motivating a line of research to offload expert weights from fast-but-small DRAM (HBM) to denser Flash SSDs. While SSDs provide cost-effective capacity, their read energy per bit is substantially higher than that of DRAM. This paper quantitatively analyzes the energy implications of offloading MoE expert weights to SSDs during the critical decode stage of LLM inference. Our analysis, comparing SSD, CPU memory (DDR), and HBM storage scenarios for models like DeepSeek-R1, reveals that offloading MoE weights to current SSDs drastically increases per-token-generation energy consumption (e.g., by up to &sim;12&times; compared to the HBM baseline), dominating the total inference energy budget. Although techniques like prefetching effectively hide access latency, they cannot mitigate this fundamental energy penalty. We further explore future technological scaling, finding that the inherent sparsity of MoE models could potentially make SSDs energy-viable if Flash read energy improves significantly, roughly by an order of magnitude.</p>
        </div>
      </section>

      <footer>
        <div class='index'><a href='202507-cal-prac.html'>&lt; prev</a> | <a href='202508-google-scholar.html'>next &gt;</a></div> 
        <a href='https://dblp.org/pid/a/JungHoAhn.html' target='_blank'><img src='https://dblp.org/img/dblp.icon.192x192.png' class='dblp'/></a>
        <a href='https://scholar.google.co.kr/citations?user=hjIKJrwAAAAJ&hl=en' target='_blank'><img src='https://upload.wikimedia.org/wikipedia/commons/c/c7/Google_Scholar_logo.svg' class='googlescholar'/></a>
        <a href='https://orcid.org/0000-0003-1733-1394' target='_blank'><img src='https://upload.wikimedia.org/wikipedia/commons/e/e6/ORCID_iD_32x32.svg' class='orcid'/></a>
        <a href='https://kr.linkedin.com/in/jung-ho-ahn-9876748' target='_blank'><img src='https://upload.wikimedia.org/wikipedia/commons/e/e8/Linkedin-logo-blue-In-square-40px.png' class='linkedin'/></a>
        <div class='copyright'>&copy; Jung Ho Ahn</div>
      </footer>

    </div>
  </body>
</html>



